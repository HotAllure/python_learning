{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Hand shape image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 initial learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define constant\n",
    "IMAGE_SIZE = 40\n",
    "COLOR_BYTE = 3\n",
    "CATEGORY_NUM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for getting hand image\n",
    "\n",
    "def load_handimage(path):\n",
    "    files = glob.glob(os.path.join(path, '*/*.png'))\n",
    "    \n",
    "    #allocate for image and label\n",
    "    images = np.ndarray((len(files), IMAGE_SIZE,IMAGE_SIZE, COLOR_BYTE),\n",
    "                       dtype = np.uint8)\n",
    "    labels = np.ndarray(len(files), dtype=np.int)\n",
    "    \n",
    "    for idx, file in enumerate(files):\n",
    "        #load image\n",
    "        image = io.imread(file)\n",
    "        images[idx] = image\n",
    "        \n",
    "        #get label name (※dir name represents label)\n",
    "        label = os.path.split(os.path.dirname(file))[-1]\n",
    "        labels[idx] = label\n",
    "    \n",
    "    #follow other datasets' format in scikit-learn\n",
    "    flat_data = images.reshape((-1, IMAGE_SIZE*IMAGE_SIZE*COLOR_BYTE))\n",
    "    images = flat_data.view()\n",
    "    return datasets.base.Bunch(data=flat_data,\n",
    "                              target=labels.astype(np.int),\n",
    "                              target_names=np.arange(CATEGORY_NUM),\n",
    "                              images=images,\n",
    "                              DESCR=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path='./data/my_learn8'\n",
    "test_path='./data/my_test2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm, metrics\n",
    "\n",
    "#you can download data from ohm HP\n",
    "\n",
    "#load dataset\n",
    "train = load_handimage(train_path)\n",
    "test = load_handimage(test_path)\n",
    "\n",
    "#instantiate model and fit\n",
    "classifier = svm.LinearSVC()\n",
    "classifier.fit(train.data, train.target)\n",
    "\n",
    "#evaluate perfornace with test data\n",
    "predicted = classifier.predict(test.data)\n",
    "\n",
    "print('Accuracy:', metrics.accuracy_score(test.target, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path='./data/my_learn10'\n",
    "test_path='./data/other_test2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "#load dataset\n",
    "train = load_handimage(train_path)\n",
    "test = load_handimage(test_path)\n",
    "\n",
    "#instantiate model and fit\n",
    "classifier = svm.LinearSVC()\n",
    "classifier.fit(train.data, train.target)\n",
    "\n",
    "#evaluate perfornace with test data\n",
    "predicted = classifier.predict(test.data)\n",
    "\n",
    "print('Accuracy:', metrics.accuracy_score(test.target, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 use more dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.2 train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/m01', './data/m02', './data/m03', './data/m04', './data/m04c', './data/m05', './data/m06', './data/m07', './data/m08', './data/m09', './data/m10', './data/m11', './data/m12', './data/m13', './data/m14', './data/m15', './data/m16']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "data_dir = './data'\n",
    "\n",
    "path_list = sorted(glob.glob(os.path.join(data_dir, '*')))\n",
    "path_list = path_list[:-4]\n",
    "\n",
    "print(path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test  ['./data/m01', './data/m05', './data/m06', './data/m07']\n",
      "train ['./data/m02', './data/m03', './data/m04', './data/m08', './data/m09', './data/m10', './data/m11', './data/m12', './data/m13', './data/m14', './data/m15', './data/m16']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_test = 4 # number of test directory\n",
    "\n",
    "'''\n",
    "# option : randomize directory\n",
    "indice = np.arange(len(path_list))\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(indice)\n",
    "\n",
    "paths_for_test = sorted(np.array(path_list)[indice[:n_test]])\n",
    "paths_for_train = sorted(np.array(path_list)[indice[n_test:]])\n",
    "'''\n",
    "\n",
    "#set directories explicitly to follow textbook\n",
    "paths_for_test =['./data/m01', './data/m05', './data/m06', './data/m07']\n",
    "paths_for_train =['./data/m02', './data/m03', './data/m04', './data/m08', './data/m09', './data/m10', \n",
    "                './data/m11', './data/m12', './data/m13', './data/m14', './data/m15', './data/m16']\n",
    "\n",
    "print('test ', paths_for_test)\n",
    "print('train', paths_for_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare train dataset\n",
    "train_data = []\n",
    "train_label = []\n",
    "\n",
    "for path in paths_for_train:\n",
    "    d = load_handimage(path)\n",
    "    train_data.append(d.data)\n",
    "    train_label.append(d.target)\n",
    "\n",
    "train_data = np.concatenate(train_data)\n",
    "train_label = np.concatenate(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#classify by SVM\n",
    "from sklearn import svm, metrics\n",
    "\n",
    "#instantiate model\n",
    "classifier = svm.LinearSVC()\n",
    "\n",
    "#fit\n",
    "classifier.fit(train_data, train_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4.3 discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ./data/m01 ###\n",
      "Accuracy: 0.7283333333333334\n",
      "Classification_Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.97      0.85       100\n",
      "          1       0.61      0.45      0.52       100\n",
      "          2       0.59      0.47      0.52       100\n",
      "          3       0.67      0.62      0.65       100\n",
      "          4       0.72      0.86      0.78       100\n",
      "          5       0.94      1.00      0.97       100\n",
      "\n",
      "avg / total       0.71      0.73      0.71       600\n",
      "\n",
      "\n",
      "### ./data/m05 ###\n",
      "Accuracy: 0.635\n",
      "Classification_Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      1.00      0.76       100\n",
      "          1       0.55      0.42      0.48       100\n",
      "          2       0.63      0.31      0.42       100\n",
      "          3       0.59      0.85      0.70       100\n",
      "          4       0.62      0.72      0.66       100\n",
      "          5       1.00      0.51      0.68       100\n",
      "\n",
      "avg / total       0.67      0.64      0.61       600\n",
      "\n",
      "\n",
      "### ./data/m06 ###\n",
      "Accuracy: 0.5933333333333334\n",
      "Classification_Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.35      0.95      0.52       100\n",
      "          1       0.72      0.39      0.51       100\n",
      "          2       0.29      0.06      0.10       100\n",
      "          3       0.72      0.83      0.77       100\n",
      "          4       0.82      0.36      0.50       100\n",
      "          5       1.00      0.97      0.98       100\n",
      "\n",
      "avg / total       0.65      0.59      0.56       600\n",
      "\n",
      "\n",
      "### ./data/m07 ###\n",
      "Accuracy: 0.6266666666666667\n",
      "Classification_Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.77      0.79       100\n",
      "          1       0.60      0.59      0.59       100\n",
      "          2       0.36      0.44      0.39       100\n",
      "          3       0.36      0.14      0.20       100\n",
      "          4       0.70      0.87      0.77       100\n",
      "          5       0.79      0.95      0.86       100\n",
      "\n",
      "avg / total       0.60      0.63      0.60       600\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate performance of model with each test dataset\n",
    "\n",
    "for path in paths_for_test:\n",
    "    d = load_handimage(path)\n",
    "    predicted = classifier.predict(d.data)\n",
    "    \n",
    "    print('### {0} ###'.format(path))\n",
    "    print('Accuracy:', metrics.accuracy_score(d.target, predicted))\n",
    "    print('Classification_Report:\\n{0}\\n'.format(metrics.classification_report(d.target, predicted)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 data cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.1 dive into data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color=\"Red\">original part : follow the discussion on textbook with working code</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### check : the reason why data/m01 got better performance than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor path in paths_for_test:\\n    d = load_handimage(path)\\n    images_2 = d.images[d.target==2]\\n    images_2 = images_2.reshape(-1 ,IMAGE_SIZE, IMAGE_SIZE, COLOR_BYTE)\\n    \\n    print('====== {0} ======'.format(path))\\n    \\n    fig = plt.figure(figsize=(40,40),dpi=100)\\n    \\n    for i, image in enumerate(images_2[:num_images]):\\n    \\n        plt.subplot(num_images/5,5,i+1)\\n        plt.imshow(image)\\n        \\n        ax = plt.gca()\\n        ax.tick_params(labelbottom=False, bottom=False)\\n        ax.tick_params(labelleft=False, left=False)\\n        \\n    plt.tight_layout()\\n    plt.show()\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_images = 50\n",
    "\n",
    "#uncomment below to show test dataset \n",
    "'''\n",
    "for path in paths_for_test:\n",
    "    d = load_handimage(path)\n",
    "    images_2 = d.images[d.target==2]\n",
    "    images_2 = images_2.reshape(-1 ,IMAGE_SIZE, IMAGE_SIZE, COLOR_BYTE)\n",
    "    \n",
    "    print('====== {0} ======'.format(path))\n",
    "    \n",
    "    fig = plt.figure(figsize=(40,40),dpi=100)\n",
    "    \n",
    "    for i, image in enumerate(images_2[:num_images]):\n",
    "    \n",
    "        plt.subplot(num_images/5,5,i+1)\n",
    "        plt.imshow(image)\n",
    "        \n",
    "        ax = plt.gca()\n",
    "        ax.tick_params(labelbottom=False, bottom=False)\n",
    "        ax.tick_params(labelleft=False, left=False)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "''' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result : images in data/m01 tend to be more regulated than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 fold cross validation score \n",
      "./data/m01 : 0.6933333333333334\n",
      "./data/m02 : 0.7066666666666667\n",
      "./data/m03 : 0.7216666666666667\n",
      "./data/m04 : 0.355\n",
      "./data/m05 : 0.6583333333333333\n",
      "./data/m06 : 0.6933333333333334\n",
      "./data/m07 : 0.6183333333333333\n",
      "./data/m08 : 0.6916666666666667\n",
      "./data/m09 : 0.5116666666666667\n",
      "./data/m10 : 0.495\n",
      "./data/m11 : 0.42833333333333334\n",
      "./data/m12 : 0.625\n",
      "./data/m13 : 0.6966666666666667\n",
      "./data/m14 : 0.6566666666666666\n",
      "./data/m15 : 0.595\n",
      "./data/m16 : 0.5833333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm, metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# prepare train dataset\n",
    "train_data = []\n",
    "train_label = []\n",
    "\n",
    "#set list of path explicitly\n",
    "path_list =['./data/m01', './data/m02', './data/m03', './data/m04', './data/m05', './data/m06', './data/m07',\n",
    "            './data/m08', './data/m09', './data/m10', './data/m11', './data/m12', './data/m13', './data/m14',\n",
    "            './data/m15', './data/m16']\n",
    "groups = []\n",
    "\n",
    "for i, path in enumerate(path_list):\n",
    "    d = load_handimage(path)\n",
    "    train_data.append(d.data)\n",
    "    train_label.append(d.target) \n",
    "    groups.append([i+1 for _ in range(len(d.data))])\n",
    "    \n",
    "train_data = np.concatenate(train_data)\n",
    "train_label = np.concatenate(train_label)\n",
    "groups = np.concatenate(groups)\n",
    "\n",
    "#instantiate model\n",
    "classifier = svm.LinearSVC()\n",
    "scores = cross_val_score(classifier, train_data, train_label, groups=groups, cv=16)\n",
    "   \n",
    "print('16 fold cross validation score ')\n",
    "for path, score in zip(path_list, scores):\n",
    "    print('{0} : {1}'.format(path, score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test data ./data/m04 got significantly low accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check : confusion matrix when using ./data/m04 as test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      " [[73 26  0  0  1  0]\n",
      " [62 36  1  0  1  0]\n",
      " [30 27  9 21 13  0]\n",
      " [22 20 12 29 17  0]\n",
      " [43 18  7  0 30  2]\n",
      " [31  4 10 22  7 26]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm, metrics\n",
    "\n",
    "#prepare train dataset\n",
    "train_data = []\n",
    "train_label = []\n",
    "\n",
    "#set list of path explicitly\n",
    "path_for_train =['./data/m01', './data/m02', './data/m03', \n",
    "                 './data/m05', './data/m06', './data/m07', './data/m08',\n",
    "                 './data/m09', './data/m10', './data/m11', './data/m12',\n",
    "                 './data/m13', './data/m14','./data/m15', './data/m16']\n",
    "\n",
    "for path in path_for_train:\n",
    "    d = load_handimage(path)\n",
    "    train_data.append(d.data)\n",
    "    train_label.append(d.target) \n",
    "    \n",
    "train_data = np.concatenate(train_data)\n",
    "train_label = np.concatenate(train_label)\n",
    "\n",
    "#prepare test dataset\n",
    "path_for_test = './data/m04'    \n",
    "\n",
    "d = load_handimage(path_for_test)\n",
    "test_data = d.data\n",
    "test_label = d.target \n",
    "\n",
    "#instantiate model\n",
    "classifier = svm.LinearSVC()\n",
    "\n",
    "#train\n",
    "classifier.fit(train_data, train_label)\n",
    "\n",
    "#predict\n",
    "predicted = classifier.predict(test_data)\n",
    "\n",
    "print('Confusion Matrix\\n', metrics.confusion_matrix(test_label, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label '2' got row performance(3 also, not written in textbook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5.2 clearning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#./data/m04 -> ./data/m04c remove noisy data\n",
    "\n",
    "#set directories explicitly to follow textbook\n",
    "paths_for_test =['./data/m01', './data/m05', './data/m06', './data/m07']\n",
    "paths_for_train =['./data/m02', './data/m03', './data/m04c', './data/m08', './data/m09', './data/m10', \n",
    "                './data/m11', './data/m12', './data/m13', './data/m14', './data/m15', './data/m16']\n",
    "\n",
    "# prepare train dataset\n",
    "train_data = []\n",
    "train_label = []\n",
    "\n",
    "for path in paths_for_train:\n",
    "    d = load_handimage(path)\n",
    "    train_data.append(d.data)\n",
    "    train_label.append(d.target)\n",
    "\n",
    "train_data = np.concatenate(train_data)\n",
    "train_label = np.concatenate(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ./data/m01 ###\n",
      "Accuracy: 0.7466666666666667\n",
      "Classification_Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.98      0.86       100\n",
      "          1       0.67      0.47      0.55       100\n",
      "          2       0.62      0.58      0.60       100\n",
      "          3       0.71      0.63      0.67       100\n",
      "          4       0.73      0.82      0.77       100\n",
      "          5       0.94      1.00      0.97       100\n",
      "\n",
      "avg / total       0.74      0.75      0.74       600\n",
      "\n",
      "\n",
      "### ./data/m05 ###\n",
      "Accuracy: 0.68\n",
      "Classification_Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.63      1.00      0.77       100\n",
      "          1       0.68      0.50      0.58       100\n",
      "          2       0.59      0.70      0.64       100\n",
      "          3       0.64      0.66      0.65       100\n",
      "          4       0.73      0.66      0.69       100\n",
      "          5       1.00      0.56      0.72       100\n",
      "\n",
      "avg / total       0.71      0.68      0.68       600\n",
      "\n",
      "\n",
      "### ./data/m06 ###\n",
      "Accuracy: 0.615\n",
      "Classification_Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.43      0.94      0.59       100\n",
      "          1       0.65      0.32      0.43       100\n",
      "          2       0.17      0.04      0.06       100\n",
      "          3       0.58      0.90      0.71       100\n",
      "          4       0.89      0.50      0.64       100\n",
      "          5       1.00      0.99      0.99       100\n",
      "\n",
      "avg / total       0.62      0.61      0.57       600\n",
      "\n",
      "\n",
      "### ./data/m07 ###\n",
      "Accuracy: 0.6333333333333333\n",
      "Classification_Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.77      0.80       100\n",
      "          1       0.55      0.57      0.56       100\n",
      "          2       0.32      0.42      0.37       100\n",
      "          3       0.42      0.20      0.27       100\n",
      "          4       0.75      0.88      0.81       100\n",
      "          5       0.89      0.96      0.92       100\n",
      "\n",
      "avg / total       0.63      0.63      0.62       600\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classify by SVM\n",
    "from sklearn import svm, metrics\n",
    "\n",
    "#instantiate model\n",
    "classifier = svm.LinearSVC()\n",
    "\n",
    "#fit\n",
    "classifier.fit(train_data, train_label)\n",
    "\n",
    "\n",
    "# evaluate performance of model with each test dataset\n",
    "\n",
    "for path in paths_for_test:\n",
    "    d = load_handimage(path)\n",
    "    predicted = classifier.predict(d.data)\n",
    "    \n",
    "    print('### {0} ###'.format(path))\n",
    "    print('Accuracy:', metrics.accuracy_score(d.target, predicted))\n",
    "    print('Classification_Report:\\n{0}\\n'.format(metrics.classification_report(d.target, predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overall performance got better, but class '2' in ./data/m06 got quite low performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 introduce HOG feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "from sklearn import datasets\n",
    "from skimage.feature import hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for getting hand image\n",
    "#image converted to hog feature \n",
    "\n",
    "'''\n",
    "description:\n",
    "\n",
    "IMAGE_HEIGHT(H) = 40\n",
    "IMAGE_WIDTH(W) = 40\n",
    "CELL_SIZE(C) = 5\n",
    "BLOCK_SIZE(B) = 5\n",
    "ORIENTATION(O) = 9\n",
    "\n",
    "HOG feature is calculated by each block.In each image, the number of block is \n",
    "(H/C - B + 1)*(W/C - B + 1) = 16\n",
    "\n",
    "Each block has 25 cells, and each cell contains HOG feature with 9 orientation.\n",
    "(0~180 deg, divided by 20)\n",
    "\n",
    "So, the entire dimension of HOG feature is \n",
    "16 * 25 * 9 = 3600\n",
    "'''\n",
    "\n",
    "CATEGORY_NUM = 6\n",
    "\n",
    "def load_handimage(path):\n",
    "    files = sorted(glob.glob(os.path.join(path, '*/*.png')))\n",
    "    \n",
    "    #allocate for image and label\n",
    "    hogs = np.ndarray((len(files), 3600), dtype = np.float)\n",
    "    labels = np.ndarray(len(files), dtype=np.int)\n",
    "    \n",
    "    for idx, file in enumerate(files):\n",
    "        #load image\n",
    "        image = io.imread(file)\n",
    "        #calculate hog feature\n",
    "        h = hog(image, orientations=9, pixels_per_cell=(5,5), cells_per_block=(5,5))\n",
    "        hogs[idx] = h\n",
    "        \n",
    "        #get label name (※dir name represents label)\n",
    "        label = os.path.split(os.path.dirname(file))[-1]\n",
    "        labels[idx] = label\n",
    "    \n",
    "    return datasets.base.Bunch(data=hogs,\n",
    "                              target=labels.astype(np.int),\n",
    "                              target_names=np.arange(CATEGORY_NUM),\n",
    "                              DESCR=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Shota_O/.pyenv/versions/anaconda3-5.0.1/envs/python_learning/lib/python3.7/site-packages/skimage/feature/_hog.py:150: skimage_deprecation: Default value of `block_norm`==`L1` is deprecated and will be changed to `L2-Hys` in v0.15. To supress this message specify explicitly the normalization method.\n",
      "  skimage_deprecation)\n"
     ]
    }
   ],
   "source": [
    "#set directories explicitly to follow textbook\n",
    "paths_for_test =['./data/m01', './data/m05', './data/m06', './data/m07']\n",
    "paths_for_train =['./data/m02', './data/m03', './data/m04', './data/m08', './data/m09', './data/m10', \n",
    "                './data/m11', './data/m12', './data/m13', './data/m14', './data/m15', './data/m16']\n",
    "\n",
    "# prepare train dataset\n",
    "train_data = []\n",
    "train_label = []\n",
    "\n",
    "for path in paths_for_train:\n",
    "    d = load_handimage(path)\n",
    "    train_data.append(d.data)\n",
    "    train_label.append(d.target)\n",
    "    \n",
    "train_data = np.concatenate(train_data)\n",
    "train_label = np.concatenate(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#classify by SVM\n",
    "from sklearn import svm, metrics\n",
    "\n",
    "#instantiate model\n",
    "classifier = svm.LinearSVC()\n",
    "\n",
    "#fit\n",
    "classifier.fit(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Shota_O/.pyenv/versions/anaconda3-5.0.1/envs/python_learning/lib/python3.7/site-packages/skimage/feature/_hog.py:150: skimage_deprecation: Default value of `block_norm`==`L1` is deprecated and will be changed to `L2-Hys` in v0.15. To supress this message specify explicitly the normalization method.\n",
      "  skimage_deprecation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ./data/m01 ###\n",
      "Accuracy: 0.8216666666666667\n",
      "Classification_Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       100\n",
      "          1       1.00      0.99      0.99       100\n",
      "          2       0.75      1.00      0.86       100\n",
      "          3       0.36      0.08      0.13       100\n",
      "          4       0.59      0.86      0.70       100\n",
      "          5       1.00      1.00      1.00       100\n",
      "\n",
      "avg / total       0.78      0.82      0.78       600\n",
      "\n",
      "\n",
      "### ./data/m05 ###\n",
      "Accuracy: 0.865\n",
      "Classification_Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       100\n",
      "          1       0.97      1.00      0.99       100\n",
      "          2       0.61      0.82      0.70       100\n",
      "          3       0.69      0.40      0.51       100\n",
      "          4       0.92      0.97      0.95       100\n",
      "          5       1.00      1.00      1.00       100\n",
      "\n",
      "avg / total       0.87      0.86      0.86       600\n",
      "\n",
      "\n",
      "### ./data/m06 ###\n",
      "Accuracy: 0.905\n",
      "Classification_Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00       100\n",
      "          1       0.97      0.98      0.98       100\n",
      "          2       1.00      0.69      0.82       100\n",
      "          3       0.66      1.00      0.79       100\n",
      "          4       0.97      0.76      0.85       100\n",
      "          5       1.00      1.00      1.00       100\n",
      "\n",
      "avg / total       0.93      0.91      0.91       600\n",
      "\n",
      "\n",
      "### ./data/m07 ###\n",
      "Accuracy: 0.7166666666666667\n",
      "Classification_Report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.96      0.89       100\n",
      "          1       0.68      0.65      0.67       100\n",
      "          2       0.49      0.43      0.46       100\n",
      "          3       0.59      0.57      0.58       100\n",
      "          4       0.75      0.72      0.73       100\n",
      "          5       0.90      0.97      0.93       100\n",
      "\n",
      "avg / total       0.71      0.72      0.71       600\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate performance of model with each test dataset\n",
    "\n",
    "for path in paths_for_test:\n",
    "    d = load_handimage(path)\n",
    "    predicted = classifier.predict(d.data)\n",
    "    \n",
    "    print('### {0} ###'.format(path))\n",
    "    print('Accuracy:', metrics.accuracy_score(d.target, predicted))\n",
    "    print('Classification_Report:\\n{0}\\n'.format(metrics.classification_report(d.target, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
